{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4986024",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ffee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/odelia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#import spacy\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.datasets import make_classification\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pycld2 as cld2\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import RocCurveDisplay, plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12306168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/WELFake_Dataset.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db641bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92022f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'].isna() & df['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'].isna() | df['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b75825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b97007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fcba2",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "## With NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fbb7c",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043308c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)           # sequences of white spaces\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)  # Removing all the non ASCII characters\n",
    "    text = re.sub(r'\\s+',' ', text)            # Replacing multiple Spaces with Single Space\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)        # Replacing Two or more dots with one\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)  # Removing all the non ASCII characters\n",
    "    text = re.sub(r'\\W+',' ', text)            # Replace everything non-alpahnumeric with a space\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad59b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].map(clean)\n",
    "df['title_clean'] = df['title'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"empty_cell_text\"] = df['text_clean'].str.contains(r'^\\s*$', na=False)\n",
    "df[\"empty_cell_title\"] = df['title_clean'].str.contains(r'^\\s*$', na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ccbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e1a96",
   "metadata": {},
   "source": [
    "### Removing empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7293250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.drop(df.loc[df[\"empty_cell_text\" or \"empty_cell_title\"]].index, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns=[\"empty_cell_text\", \"empty_cell_title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/df_cleaned_nonan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c04487",
   "metadata": {},
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425677e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    _, _, _, detected_language = cld2.detect(text, returnVectors=True)\n",
    "    return str(detected_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45128b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lang'] = df['text_clean'].map(detect_lang)\n",
    "df['title_lang'] = df['title_clean'].map(detect_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4579d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text_lang'].astype(str)\n",
    "df['title_lang'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lang'] = ~df[\"text_lang\"].str.contains('ENGLISH|Unknown', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_lang'] = ~df[\"title_lang\"].str.contains('ENGLISH|Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d158929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df[\"text_lang\" or \"title_lang\"]].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49241ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0343ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db552c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/df_pre_tok.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9139d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c9a19",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be23080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "df['token_title'] = df.apply(lambda row: nltk.word_tokenize(row['title_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text'] = df.apply(lambda row: nltk.word_tokenize(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"text_lang\", \"title_lang\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/df_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6de81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"df_token.csv\")\n",
    "#df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b51ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6072f",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80892b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e79a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['tag_title'] = df.apply(lambda row: nltk.pos_tag(row['token_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['tag_text'] = df.apply(lambda row: nltk.pos_tag(row['token_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tag_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad49f9",
   "metadata": {},
   "source": [
    "### Lemmatizing tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    lemmatized_words = [lem.lemmatize(word) for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lem_title'] = df.apply(lambda row: lemmatize(row['token_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64602d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lem_text'] = df.apply(lambda row: lemmatize(row['token_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/df_lemmatized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183891b5",
   "metadata": {},
   "source": [
    "for word, tag in enumerate(df['tag_title']):\n",
    "         wntag = tag[0][0][0].lower()\n",
    "         wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "         lemma = lem.lemmatize(word, wntag) if wntag else word\n",
    "         print (lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1e257",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6934b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d96650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = re.sub(r\"\\s*'\\s*\\w*\",\"\",stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stop_title\"] = df[\"lem_title\"].apply(lambda x: ' '.join([word for word in x if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stop_text\"] = df[\"lem_text\"].apply(lambda x: ' '.join([word for word in x if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fe215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/df_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c42115",
   "metadata": {},
   "source": [
    "### BOW with countvec [ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer(max_features=100)\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return (vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(vectorizer, X) = create_vectorizer(df.stop_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c77fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "denseX = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "denseX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe594008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a647aea",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f7cf4",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on titles\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['stop_title'],\n",
    "df['label'],\n",
    "test_size=0.2,\n",
    "random_state=42,\n",
    "stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Size of Training Data ', X_train.shape[0])\n",
    "print ('Size of Test Data ', X_test.shape[0])\n",
    "print ('Distribution of classes in Training Data :')\n",
    "print ('Fake item ', str(sum(Y_train == 1)/ len(Y_train) * 100.0))\n",
    "print ('Real item ', str(sum(Y_train == 0)/ len(Y_train) * 100.0))\n",
    "print ('Distribution of classes in Testing Data :')\n",
    "print ('Fake item ', str(sum(Y_test == 1)/ len(Y_test) * 100.0))\n",
    "print ('Real item ', str(sum(Y_test == 0)/ len(Y_test) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 20000, ngram_range=(1,2))\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ece6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = LinearSVC(random_state=42, tol=1e-5)\n",
    "SVC.fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea48532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = SVC.predict(X_test_tf)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print ('ROC-AUC Score - ', roc_auc_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on text\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['stop_text'],\n",
    "df['label'],\n",
    "test_size=0.2,\n",
    "random_state=42,\n",
    "stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc33263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Size of Training Data ', X_train.shape[0])\n",
    "print ('Size of Test Data ', X_test.shape[0])\n",
    "print ('Distribution of classes in Training Data :')\n",
    "print ('Fake item ', str(sum(Y_train == 1)/ len(Y_train) * 100.0))\n",
    "print ('Real item ', str(sum(Y_train == 0)/ len(Y_train) * 100.0))\n",
    "print ('Distribution of classes in Testing Data :')\n",
    "print ('Fake item ', str(sum(Y_test == 1)/ len(Y_test) * 100.0))\n",
    "print ('Real item ', str(sum(Y_test == 0)/ len(Y_test) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f46676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer = 'word', max_features = 20000, ngram_range=(1,2))\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b28f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = LinearSVC(random_state=42, tol=1e-5)\n",
    "SVC.fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = SVC.predict(X_test_tf)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print ('ROC-AUC Score - ', roc_auc_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97081fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7955fe9",
   "metadata": {},
   "source": [
    "## Pretrained models and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261bcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55ac53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.0020e-01, 9.9045e-01, 2.9815e-01],\n",
      "        [8.2319e-01, 7.1542e-01, 9.3503e-01],\n",
      "        [2.9318e-01, 1.5846e-01, 8.7136e-04],\n",
      "        [6.0940e-01, 6.3748e-01, 6.6152e-01],\n",
      "        [4.2681e-01, 2.2537e-01, 4.2717e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fbf0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased',finetuning_task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085b6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e22468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True): \n",
    "    input_ids = tokenizer.encode(text, \n",
    "                                 add_special_tokens=add_special_tokens, \n",
    "                                 max_length=max_seq_length, \n",
    "                                 pad_to_max_length=True) \n",
    "    attention_mask = [int(id > 0) for id in input_ids] \n",
    "    assert len(input_ids) == max_seq_length \n",
    "    assert len(attention_mask) == max_seq_length \n",
    "    return (input_ids, attention_mask) \n",
    "text = \"Here is the sentence I want embeddings for.\" \n",
    "input_ids, attention_mask = get_tokens(text, \n",
    "                                       tokenizer, \n",
    "                                       max_seq_length=30, \n",
    "                                       add_special_tokens = True) \n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids) \n",
    "print (text) \n",
    "print (input_tokens) \n",
    "print (input_ids) \n",
    "print (attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd9513d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_pre_tok.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_pre_tok.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XY train_test split + tokenize X_train and X_test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['title_clean'],\n",
    "                                                    df['label'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=df['label'])\n",
    "\n",
    "X_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 50)) \n",
    "X_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of 3 tensors: tokens, input masks and target labels\n",
    "\n",
    "from torch.utils.data import TensorDataset \n",
    "\n",
    "input_ids_train = torch.tensor( \n",
    "    [features[0] for features in X_train_tokens.values], dtype=torch.long) \n",
    "input_mask_train = torch.tensor( \n",
    "    [features[1] for features in X_train_tokens.values], dtype=torch.long) \n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long) \n",
    "\n",
    "print (input_ids_train.shape) \n",
    "print (input_mask_train.shape) \n",
    "print (label_ids_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45960826",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine tensors into a tensordataset\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train,input_mask_train,label_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb48593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_batch_size = 64\n",
    "num_train_epochs = 2\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "sampler=train_sampler,\n",
    "batch_size=train_batch_size)\n",
    "t_total = len(train_dataloader) // num_train_epochs\n",
    "print (\"Num examples = \", len(train_dataset))\n",
    "print (\"Num Epochs = \", num_train_epochs)\n",
    "print (\"Total train batch size = \", train_batch_size)\n",
    "print (\"Total optimization steps = \", t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-4\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "num_warmup_steps=warmup_steps,\n",
    "num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f557817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd12361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
